时间：2021-09-10
概述：3个节点的nacos集群突然无法提供服务，导致全网不稳定
过程描述：
  1.nacos3个节点的console界面都难以进去，接口请求很久才有响应。
  2.3个节点的gc都不正常，jvm的配置是4g堆内存2g年轻代，没有设置垃圾回收器，默认回收器是ps+pn。
  3.重新设置jvm参数后（修改堆内存为8g，回收器为g1），垃圾回收趋于正常，但cpu飙到300%以上（服务器4核）
  4.服务器升级配置以后（16g，8核）陆续重启nacos节点（始终保持至少一个节点在线）后，进入console仍然很卡。
  5.在此期间不断重启nacos服务，也检查过配置，甚至重新拉包重启，皆无效。
  6.公司技术总监远程请了所谓的spring cloud中国社区创始人和nacos开源作者来帮忙，无果。
  7.console进入很慢，在此期间3台nacos都启动的情况下在集群管理中看到的节点只能看到当前节点是up其他节点都是down状态。
  8.观察了3台nacos集群节点信息其中的term、leader信息，发现存在leader节点是别的ip，但那个ip的节点状态为down。
  9.观察了3台nacos，分别都建立了超过8000个连接，起初我怀疑是连接太多导致响应太慢，于是问了所谓的外援他们的nacos一般是建立多少连接，被怼。
  10.后续继续观察了集群信息，发现leaderip经常变化，且leader的ip一般都在其他节点显示为down状态，我断定此时nacos集群内部通信出现了问题
  11.这个问题我归纳为nacos集群内部通信问题，但目前尚不能确定是nacos选举造成的接口响应慢，还是接口响应慢造成了不断选举。
  12.只要nacos在不断进行选举，那么对外提供服务就会存在问题，因为服务的心跳、注册、拉取等服务，需要同步请求主节点，异步请求从节点同步数据。
  13.这时候我想如果集群内只剩一个节点，不存在集群同步数据，不存在选举，那么一个节点能否正常提供服务。
  14.执行操作，将nacos3个节点关闭2个，只剩一个节点，服务恢复正常，业务恢复正常。

后续研究：
  1.准备去其他环境复现问题，但不管是1.4.0、1.4.1、1.4.2都不能复现问题
  2.nacos-client与nacos-server建立的是长链接，nacos节点内部同步数据是短连接，每次nacos-client请求nacos-server的同步心跳都会请求其他的nacos节点同步数据，导致了大量的TIME_WAIT连接，如果nacos是单独部署的服务器，那么可以把ipv4的recycle打开，可以快速回收time_wait的连接。
  3.nacos-client不管是dubbo还是springcloud如果配置了集群多节点，那么都会随机负载，每次请求都会随机与集群内任意节点建立长链接。
  4.基于第3点，改进是利用nginx对nacos集群做代理，让服务只请求一个nginx的地址（nginx也加keepalived或者直接用slb），那么建立的长连接数才是正常的。
  5.springcloud和dubbo注册到nacos量级不一样，springcloud是http，也就是应用级，一个java程序只会注册一个服务上去，而dubbo，一个接口理论上会注册一个provider，一个或多个consumer到nacos，所以建立的连接和量级肯定是dubbo更大。
  6.这个问题怀疑是nacos的bug（1.4.1），在1.4.2中能看到有几个raft相关的bugfix，但尚无法确定是否修复，且还没复现。
  7.曾经使用nacos1.4.0，正常的杀掉重启能恢复正常，但几次服务器断电后重启服务就报错，删除了raft缓存才能启动。
